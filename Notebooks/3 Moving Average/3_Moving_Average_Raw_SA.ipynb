{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from collections import defaultdict\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataset (Outputs: ids, trial_nums, predictors_df, outcomes_df, outcomes_df_shuffled)\n",
    "# Import dataset\n",
    "df = pd.read_csv(\"./../../Datasets/kieranFeatures_1-31_21-Jan-2025_avgof3_rawSA.csv\").replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Create Low vs High Columns\n",
    "df[\"Lv_1_Lo\"] = (df[\"SA1\"] < 5).astype(np.bool_)\n",
    "df[\"Lv_2_Lo\"] = (df[\"SA2\"] < 5).astype(np.bool_)\n",
    "df[\"Lv_3_Lo\"] = (df[\"SA3\"] < 5).astype(np.bool_)\n",
    "df[\"Tot_Lo\"] = (df[\"SAtotal\"] < 15).astype(np.bool_)\n",
    "\n",
    "# Impute missing values with mean of column\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].fillna(value = df[col].mean())\n",
    "\n",
    "# Split up dataset\n",
    "ids = df[\"ID\"].astype(np.uint8)\n",
    "# trial_nums = df[\"trialNum\"].astype(np.uint8)\n",
    "predictors_df = df.drop(columns = [\"ID\", \"trialNum\", \"SA1\", \"SA2\", \"SA3\", \"SAtotal\", \"Lv_1_Lo\", \"Lv_2_Lo\", \"Lv_3_Lo\", \"Tot_Lo\"]).astype(np.float64)\n",
    "outcomes_df = df[[\"Lv_1_Lo\", \"Lv_2_Lo\", \"Lv_3_Lo\", \"Tot_Lo\"]]\n",
    "outcomes_df_shuffled = outcomes_df.copy()\n",
    "\n",
    "# Shuffle labels for shuffled data\n",
    "outcomes_df_shuffled[\"Lv_1_Lo\"] = np.random.permutation(outcomes_df_shuffled[\"Lv_1_Lo\"])\n",
    "outcomes_df_shuffled[\"Lv_2_Lo\"] = np.random.permutation(outcomes_df_shuffled[\"Lv_2_Lo\"])\n",
    "outcomes_df_shuffled[\"Lv_3_Lo\"] = np.random.permutation(outcomes_df_shuffled[\"Lv_3_Lo\"])\n",
    "outcomes_df_shuffled[\"Tot_Lo\"] = np.random.permutation(outcomes_df_shuffled[\"Tot_Lo\"])\n",
    "\n",
    "# Free up memory\n",
    "del col, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize f1_scores, accuracy_scores, CV_models, and modified_Ridge_features\n",
    "f1_scores = {\n",
    "    \"Actual\": {\n",
    "        \"Lv_1_Lo\": [],\n",
    "        \"Lv_2_Lo\": [],\n",
    "        \"Lv_3_Lo\": [],\n",
    "        \"Tot_Lo\": []\n",
    "    },\n",
    "    \"Shuffled\": {\n",
    "        \"Lv_1_Lo\": [],\n",
    "        \"Lv_2_Lo\": [],\n",
    "        \"Lv_3_Lo\": [],\n",
    "        \"Tot_Lo\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "accuracy_scores = {\n",
    "    \"Actual\": {\n",
    "        \"Lv_1_Lo\": [],\n",
    "        \"Lv_2_Lo\": [],\n",
    "        \"Lv_3_Lo\": [],\n",
    "        \"Tot_Lo\": []\n",
    "    },\n",
    "    \"Shuffled\": {\n",
    "        \"Lv_1_Lo\": [],\n",
    "        \"Lv_2_Lo\": [],\n",
    "        \"Lv_3_Lo\": [],\n",
    "        \"Tot_Lo\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "CV_models = {\n",
    "    \"Actual\": {\n",
    "        \"Lv_1_Lo\": [],\n",
    "        \"Lv_2_Lo\": [],\n",
    "        \"Lv_3_Lo\": [],\n",
    "        \"Tot_Lo\": []\n",
    "    },\n",
    "    \"Shuffled\": {\n",
    "        \"Lv_1_Lo\": [],\n",
    "        \"Lv_2_Lo\": [],\n",
    "        \"Lv_3_Lo\": [],\n",
    "        \"Tot_Lo\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "modified_Ridge_features = {\n",
    "    \"Actual\": {\n",
    "        \"Lv_1_Lo\": [],\n",
    "        \"Lv_2_Lo\": [],\n",
    "        \"Lv_3_Lo\": [],\n",
    "        \"Tot_Lo\": []\n",
    "    },\n",
    "    \"Shuffled\": {\n",
    "        \"Lv_1_Lo\": [],\n",
    "        \"Lv_2_Lo\": [],\n",
    "        \"Lv_3_Lo\": [],\n",
    "        \"Tot_Lo\": []\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(pred_df, out_df, ids):\n",
    "    f1_scores = defaultdict(list)\n",
    "    accuracy_scores = defaultdict(list)\n",
    "    models = defaultdict(list)\n",
    "    modified_Ridge_selected_features = []\n",
    "\n",
    "    # Obtain 10 test folds stratifying by participant ID\n",
    "    skf = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "    for i, (CV_idx, test_idx) in enumerate(skf.split(pred_df, ids)):\n",
    "        # Train-Test Split for the Fold\n",
    "        pred_CV = pred_df.iloc[CV_idx, :].values\n",
    "        pred_test = pred_df.iloc[test_idx, :].values\n",
    "        out_CV = out_df.iloc[CV_idx].values\n",
    "        out_test = out_df.iloc[test_idx].values\n",
    "\n",
    "        # Standardize Data\n",
    "        scaler = StandardScaler()\n",
    "        pred_CV_normalized = scaler.fit_transform(pred_CV)\n",
    "        pred_test_normalized = scaler.transform(pred_test)\n",
    "\n",
    "        # Free Up Memory\n",
    "        del pred_CV\n",
    "        del pred_test\n",
    "        del out_CV\n",
    "        del pred_CV_normalized\n",
    "\n",
    "        # Initialize Models to Train\n",
    "        no_penalty_model = LogisticRegression(\n",
    "            fit_intercept = False,\n",
    "            solver = \"saga\",\n",
    "            n_jobs = -1,\n",
    "            max_iter = 20000,\n",
    "            class_weight = \"balanced\",\n",
    "            penalty = None,\n",
    "            random_state = 42\n",
    "        )\n",
    "\n",
    "        Ridge_model = LogisticRegression(\n",
    "            C = 0.001,\n",
    "            fit_intercept = False,\n",
    "            solver = \"saga\",\n",
    "            n_jobs = -1,\n",
    "            max_iter = 20000,\n",
    "            class_weight = \"balanced\",\n",
    "            penalty = \"l2\",\n",
    "            random_state = 42\n",
    "        )\n",
    "\n",
    "        LASSO_model = LogisticRegression(\n",
    "            C = 1,\n",
    "            fit_intercept = False,\n",
    "            solver = \"saga\",\n",
    "            n_jobs = -1,\n",
    "            max_iter = 20000,\n",
    "            class_weight = \"balanced\",\n",
    "            penalty = \"l1\",\n",
    "            random_state = 42\n",
    "        )\n",
    "\n",
    "        modified_Ridge_model_l2 = LogisticRegression(\n",
    "            fit_intercept = False,\n",
    "            solver = \"saga\",\n",
    "            n_jobs = -1,\n",
    "            max_iter = 20000,\n",
    "            class_weight = \"balanced\",\n",
    "            penalty = \"l2\",\n",
    "            C = 0.001\n",
    "        )\n",
    "\n",
    "        # Fit Models\n",
    "        no_penalty_model.fit(pred_CV_normalized, out_CV)\n",
    "        Ridge_model.fit(pred_CV_normalized, out_CV)\n",
    "        LASSO_model.fit(pred_CV_normalized, out_CV)\n",
    "\n",
    "        # Obtain and fit \"Modified Ridge\" Model\n",
    "        Ridge_selector = SelectFromModel(Ridge_model, prefit = True)\n",
    "        Ridge_selected_features = Ridge_selector.get_support()\n",
    "        pred_CV_selected_Ridge = pred_CV_normalized[:, Ridge_selected_features]\n",
    "        pred_test_selected_Ridge = pred_test_normalized[:, Ridge_selected_features]\n",
    "        modified_Ridge_model_l2.fit(pred_CV_selected_Ridge, out_CV)\n",
    "\n",
    "        # Append F1 Scores\n",
    "        f1_scores[\"No Penalty\"].append(f1_score(out_test, no_penalty_model.predict(pred_test_normalized)))\n",
    "        f1_scores[\"Ridge\"].append(f1_score(out_test, Ridge_model.predict(pred_test_normalized)))\n",
    "        f1_scores[\"LASSO\"].append(f1_score(out_test, LASSO_model.predict(pred_test_normalized)))\n",
    "        f1_scores[\"Modified Ridge (L2)\"].append(f1_score(out_test, modified_Ridge_model_l2.predict(pred_test_selected_Ridge)))\n",
    "\n",
    "        # Append accuracy Scores\n",
    "        accuracy_scores[\"No Penalty\"].append(accuracy_score(out_test, no_penalty_model.predict(pred_test_normalized)))\n",
    "        accuracy_scores[\"Ridge\"].append(accuracy_score(out_test, Ridge_model.predict(pred_test_normalized)))\n",
    "        accuracy_scores[\"LASSO\"].append(accuracy_score(out_test, LASSO_model.predict(pred_test_normalized)))\n",
    "        accuracy_scores[\"Modified Ridge (L2)\"].append(accuracy_score(out_test, modified_Ridge_model_l2.predict(pred_test_selected_Ridge)))\n",
    "\n",
    "        # Add Models and Scores to Dictionaries\n",
    "        models[\"No Penalty\"].append(no_penalty_model)\n",
    "        models[\"Ridge\"].append(Ridge_model)\n",
    "        models[\"LASSO\"].append(LASSO_model)\n",
    "        models[\"Modified Ridge (L2)\"].append(modified_Ridge_model_l2)\n",
    "\n",
    "        # Store selected features for relaxed LASSO\n",
    "        modified_Ridge_selected_features.append(Ridge_selected_features)\n",
    "\n",
    "    return f1_scores, accuracy_scores, models, modified_Ridge_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for outcome_var in [\"Lv_1_Lo\", \"Lv_2_Lo\", \"Lv_3_Lo\", \"Tot_Lo\"]:\n",
    "    for outcome_type in [\"Actual\", \"Shuffled\"]:\n",
    "        if outcome_type == \"Actual\":\n",
    "            f1_scores[outcome_type][outcome_var], accuracy_scores[outcome_type][outcome_var], CV_models[outcome_type][outcome_var], modified_Ridge_features[outcome_type][outcome_var] = evaluate_models(predictors_df, outcomes_df[outcome_var], ids)\n",
    "        else:\n",
    "            f1_scores[outcome_type][outcome_var], accuracy_scores[outcome_type][outcome_var], CV_models[outcome_type][outcome_var], modified_Ridge_features[outcome_type][outcome_var] = evaluate_models(predictors_df, outcomes_df_shuffled[outcome_var], ids)\n",
    "        print(\"Completed Training for\", outcome_type, outcome_var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
