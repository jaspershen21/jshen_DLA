{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from collections import defaultdict\n",
    "import scipy.stats as stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataset (Outputs: ids, trial_nums, predictors_df, outcomes_df, outcomes_df_shuffled)\n",
    "# Import dataset\n",
    "df = pd.read_csv(\"./../../Datasets/kieranFeatures_1-31_21-Jan-2025_avgof3_rawSA.csv\").replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Create Low vs High Columns\n",
    "df[\"Lv_1_Lo\"] = (df[\"SA1\"] < 5).astype(np.bool_)\n",
    "df[\"Lv_2_Lo\"] = (df[\"SA2\"] < 5).astype(np.bool_)\n",
    "df[\"Lv_3_Lo\"] = (df[\"SA3\"] < 5).astype(np.bool_)\n",
    "df[\"Tot_Lo\"] = (df[\"SAtotal\"] < 15).astype(np.bool_)\n",
    "\n",
    "# Impute missing values with mean of column\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].fillna(value = df[col].mean())\n",
    "\n",
    "# Split up dataset\n",
    "ids = df[\"ID\"].astype(np.uint8)\n",
    "# trial_nums = df[\"trialNum\"].astype(np.uint8)\n",
    "predictors_df = df.drop(columns = [\"ID\", \"trialNum\", \"SA1\", \"SA2\", \"SA3\", \"SAtotal\", \"Lv_1_Lo\", \"Lv_2_Lo\", \"Lv_3_Lo\", \"Tot_Lo\"]).astype(np.float64)\n",
    "outcomes_df = df[[\"Lv_1_Lo\", \"Lv_2_Lo\", \"Lv_3_Lo\", \"Tot_Lo\"]]\n",
    "outcomes_df_shuffled = outcomes_df.copy()\n",
    "\n",
    "# Shuffle labels for shuffled data\n",
    "outcomes_df_shuffled[\"Lv_1_Lo\"] = np.random.permutation(outcomes_df_shuffled[\"Lv_1_Lo\"])\n",
    "outcomes_df_shuffled[\"Lv_2_Lo\"] = np.random.permutation(outcomes_df_shuffled[\"Lv_2_Lo\"])\n",
    "outcomes_df_shuffled[\"Lv_3_Lo\"] = np.random.permutation(outcomes_df_shuffled[\"Lv_3_Lo\"])\n",
    "outcomes_df_shuffled[\"Tot_Lo\"] = np.random.permutation(outcomes_df_shuffled[\"Tot_Lo\"])\n",
    "\n",
    "# Free up memory\n",
    "del col, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize f1_scores, accuracy_scores, CV_models, and modified_Ridge_features\n",
    "f1_scores = {\n",
    "    \"Actual\": {\n",
    "        \"Lv_1_Lo\": [],\n",
    "        \"Lv_2_Lo\": [],\n",
    "        \"Lv_3_Lo\": [],\n",
    "        \"Tot_Lo\": []\n",
    "    },\n",
    "    \"Shuffled\": {\n",
    "        \"Lv_1_Lo\": [],\n",
    "        \"Lv_2_Lo\": [],\n",
    "        \"Lv_3_Lo\": [],\n",
    "        \"Tot_Lo\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "accuracy_scores = {\n",
    "    \"Actual\": {\n",
    "        \"Lv_1_Lo\": [],\n",
    "        \"Lv_2_Lo\": [],\n",
    "        \"Lv_3_Lo\": [],\n",
    "        \"Tot_Lo\": []\n",
    "    },\n",
    "    \"Shuffled\": {\n",
    "        \"Lv_1_Lo\": [],\n",
    "        \"Lv_2_Lo\": [],\n",
    "        \"Lv_3_Lo\": [],\n",
    "        \"Tot_Lo\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "CV_models = {\n",
    "    \"Actual\": {\n",
    "        \"Lv_1_Lo\": [],\n",
    "        \"Lv_2_Lo\": [],\n",
    "        \"Lv_3_Lo\": [],\n",
    "        \"Tot_Lo\": []\n",
    "    },\n",
    "    \"Shuffled\": {\n",
    "        \"Lv_1_Lo\": [],\n",
    "        \"Lv_2_Lo\": [],\n",
    "        \"Lv_3_Lo\": [],\n",
    "        \"Tot_Lo\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "modified_Ridge_features = {\n",
    "    \"Actual\": {\n",
    "        \"Lv_1_Lo\": [],\n",
    "        \"Lv_2_Lo\": [],\n",
    "        \"Lv_3_Lo\": [],\n",
    "        \"Tot_Lo\": []\n",
    "    },\n",
    "    \"Shuffled\": {\n",
    "        \"Lv_1_Lo\": [],\n",
    "        \"Lv_2_Lo\": [],\n",
    "        \"Lv_3_Lo\": [],\n",
    "        \"Tot_Lo\": []\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(pred_df, out_df, ids):\n",
    "    f1_scores = defaultdict(list)\n",
    "    accuracy_scores = defaultdict(list)\n",
    "    models = defaultdict(list)\n",
    "    modified_Ridge_selected_features = []\n",
    "\n",
    "    # Obtain 10 test folds stratifying by participant ID\n",
    "    skf = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "    for i, (CV_idx, test_idx) in enumerate(skf.split(pred_df, ids)):\n",
    "        # Train-Test Split for the Fold\n",
    "        pred_CV = pred_df.iloc[CV_idx, :].values\n",
    "        pred_test = pred_df.iloc[test_idx, :].values\n",
    "        out_CV = out_df.iloc[CV_idx].values\n",
    "        out_test = out_df.iloc[test_idx].values\n",
    "\n",
    "        # Standardize Data\n",
    "        scaler = StandardScaler()\n",
    "        pred_CV_normalized = scaler.fit_transform(pred_CV)\n",
    "        pred_test_normalized = scaler.transform(pred_test)\n",
    "\n",
    "        # Free Up Memory\n",
    "        del pred_CV\n",
    "        del pred_test\n",
    "\n",
    "        # Initialize Models to Train\n",
    "        no_penalty_model = LogisticRegression(\n",
    "            fit_intercept = False,\n",
    "            solver = \"saga\",\n",
    "            n_jobs = -1,\n",
    "            max_iter = 20000,\n",
    "            class_weight = \"balanced\",\n",
    "            penalty = None,\n",
    "            random_state = 42\n",
    "        )\n",
    "\n",
    "        Ridge_model = LogisticRegression(\n",
    "            C = 0.001,\n",
    "            fit_intercept = False,\n",
    "            solver = \"saga\",\n",
    "            n_jobs = -1,\n",
    "            max_iter = 20000,\n",
    "            class_weight = \"balanced\",\n",
    "            penalty = \"l2\",\n",
    "            random_state = 42\n",
    "        )\n",
    "\n",
    "        LASSO_model = LogisticRegression(\n",
    "            C = 1,\n",
    "            fit_intercept = False,\n",
    "            solver = \"saga\",\n",
    "            n_jobs = -1,\n",
    "            max_iter = 20000,\n",
    "            class_weight = \"balanced\",\n",
    "            penalty = \"l1\",\n",
    "            random_state = 42\n",
    "        )\n",
    "\n",
    "        modified_Ridge_model_l2 = LogisticRegression(\n",
    "            fit_intercept = False,\n",
    "            solver = \"saga\",\n",
    "            n_jobs = -1,\n",
    "            max_iter = 20000,\n",
    "            class_weight = \"balanced\",\n",
    "            penalty = \"l2\",\n",
    "            C = 0.001\n",
    "        )\n",
    "\n",
    "        # Fit Models\n",
    "        no_penalty_model.fit(pred_CV_normalized, out_CV)\n",
    "        Ridge_model.fit(pred_CV_normalized, out_CV)\n",
    "        LASSO_model.fit(pred_CV_normalized, out_CV)\n",
    "\n",
    "        # Obtain and fit \"Modified Ridge\" Model\n",
    "        Ridge_selector = SelectFromModel(Ridge_model, prefit = True)\n",
    "        Ridge_selected_features = Ridge_selector.get_support()\n",
    "        pred_CV_selected_Ridge = pred_CV_normalized[:, Ridge_selected_features]\n",
    "        pred_test_selected_Ridge = pred_test_normalized[:, Ridge_selected_features]\n",
    "        modified_Ridge_model_l2.fit(pred_CV_selected_Ridge, out_CV)\n",
    "\n",
    "        # Append F1 Scores\n",
    "        f1_scores[\"No Penalty\"].append(f1_score(out_test, no_penalty_model.predict(pred_test_normalized)))\n",
    "        f1_scores[\"Ridge\"].append(f1_score(out_test, Ridge_model.predict(pred_test_normalized)))\n",
    "        f1_scores[\"LASSO\"].append(f1_score(out_test, LASSO_model.predict(pred_test_normalized)))\n",
    "        f1_scores[\"Modified Ridge (L2)\"].append(f1_score(out_test, modified_Ridge_model_l2.predict(pred_test_selected_Ridge)))\n",
    "\n",
    "        # Append accuracy Scores\n",
    "        accuracy_scores[\"No Penalty\"].append(accuracy_score(out_test, no_penalty_model.predict(pred_test_normalized)))\n",
    "        accuracy_scores[\"Ridge\"].append(accuracy_score(out_test, Ridge_model.predict(pred_test_normalized)))\n",
    "        accuracy_scores[\"LASSO\"].append(accuracy_score(out_test, LASSO_model.predict(pred_test_normalized)))\n",
    "        accuracy_scores[\"Modified Ridge (L2)\"].append(accuracy_score(out_test, modified_Ridge_model_l2.predict(pred_test_selected_Ridge)))\n",
    "\n",
    "        # Add Models and Scores to Dictionaries\n",
    "        models[\"No Penalty\"].append(no_penalty_model)\n",
    "        models[\"Ridge\"].append(Ridge_model)\n",
    "        models[\"LASSO\"].append(LASSO_model)\n",
    "        models[\"Modified Ridge (L2)\"].append(modified_Ridge_model_l2)\n",
    "\n",
    "        # Store selected features for relaxed LASSO\n",
    "        modified_Ridge_selected_features.append(Ridge_selected_features)\n",
    "\n",
    "    return f1_scores, accuracy_scores, models, modified_Ridge_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 6 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m outcome_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActual\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShuffled\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m outcome_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActual\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m         f1_scores[outcome_type][outcome_var], accuracy_scores[outcome_type][outcome_var], CV_models[outcome_type][outcome_var], modified_Ridge_features[outcome_type][outcome_var] \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictors_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutcomes_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43moutcome_var\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m         f1_scores[outcome_type][outcome_var], accuracy_scores[outcome_type][outcome_var], CV_models[outcome_type][outcome_var], modified_Ridge_features[outcome_type][outcome_var] \u001b[38;5;241m=\u001b[39m evaluate_models(predictors_df, outcomes_df_shuffled[outcome_var], ids)\n",
      "Cell \u001b[0;32mIn[8], line 69\u001b[0m, in \u001b[0;36mevaluate_models\u001b[0;34m(pred_df, out_df, ids)\u001b[0m\n\u001b[1;32m     58\u001b[0m modified_Ridge_model_l2 \u001b[38;5;241m=\u001b[39m LogisticRegression(\n\u001b[1;32m     59\u001b[0m     fit_intercept \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     60\u001b[0m     solver \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m     C \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m     66\u001b[0m )\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Fit Models\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[43mno_penalty_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_CV_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_CV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m Ridge_model\u001b[38;5;241m.\u001b[39mfit(pred_CV_normalized, out_CV)\n\u001b[1;32m     71\u001b[0m LASSO_model\u001b[38;5;241m.\u001b[39mfit(pred_CV_normalized, out_CV)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1350\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1348\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1350\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for outcome_var in [\"Lv_1_Lo\", \"Lv_2_Lo\", \"Lv_3_Lo\", \"Tot_Lo\"]:\n",
    "    for outcome_type in [\"Actual\", \"Shuffled\"]:\n",
    "        if outcome_type == \"Actual\":\n",
    "            f1_scores[outcome_type][outcome_var], accuracy_scores[outcome_type][outcome_var], CV_models[outcome_type][outcome_var], modified_Ridge_features[outcome_type][outcome_var] = evaluate_models(predictors_df, outcomes_df[outcome_var], ids)\n",
    "        else:\n",
    "            f1_scores[outcome_type][outcome_var], accuracy_scores[outcome_type][outcome_var], CV_models[outcome_type][outcome_var], modified_Ridge_features[outcome_type][outcome_var] = evaluate_models(predictors_df, outcomes_df_shuffled[outcome_var], ids)\n",
    "        print(\"Completed Training for\", outcome_type, outcome_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
