{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>EDA_TonicMean_version02</th>\n",
       "      <th>EDA_TonicMean_version03</th>\n",
       "      <th>EDA_TonicMean_version04</th>\n",
       "      <th>EDA_TonicMean_version05</th>\n",
       "      <th>EDA_TonicMean_version09</th>\n",
       "      <th>EDA_TonicMean_version10</th>\n",
       "      <th>EDA_TonicMean_version11</th>\n",
       "      <th>EDA_TonicMean_version12</th>\n",
       "      <th>EDA_TonicMean_version16</th>\n",
       "      <th>...</th>\n",
       "      <th>EEG_avgRelTheta_version16</th>\n",
       "      <th>EEG_avgRelTheta_version17</th>\n",
       "      <th>EEG_avgRelTheta_version19</th>\n",
       "      <th>EEG_avgRelTheta_version20</th>\n",
       "      <th>EEG_avgRelTheta_version22</th>\n",
       "      <th>EEG_avgRelTheta_version23</th>\n",
       "      <th>adjSA1</th>\n",
       "      <th>adjSA2</th>\n",
       "      <th>adjSA3</th>\n",
       "      <th>adjSAtotal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.123031</td>\n",
       "      <td>-0.226077</td>\n",
       "      <td>-1.220480</td>\n",
       "      <td>-1.697738</td>\n",
       "      <td>-0.273200</td>\n",
       "      <td>-0.601171</td>\n",
       "      <td>-0.809518</td>\n",
       "      <td>-1.012558</td>\n",
       "      <td>-0.299118</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.877017</td>\n",
       "      <td>-1.442056</td>\n",
       "      <td>1.070298</td>\n",
       "      <td>1.277417</td>\n",
       "      <td>0.249605</td>\n",
       "      <td>0.400156</td>\n",
       "      <td>0.119790</td>\n",
       "      <td>1.593122</td>\n",
       "      <td>-0.800726</td>\n",
       "      <td>0.350233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.152896</td>\n",
       "      <td>-0.050866</td>\n",
       "      <td>1.527067</td>\n",
       "      <td>1.883468</td>\n",
       "      <td>-0.378060</td>\n",
       "      <td>-0.018812</td>\n",
       "      <td>1.023216</td>\n",
       "      <td>1.189124</td>\n",
       "      <td>-0.355315</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.632698</td>\n",
       "      <td>-1.531970</td>\n",
       "      <td>1.779032</td>\n",
       "      <td>1.074498</td>\n",
       "      <td>0.409991</td>\n",
       "      <td>0.333842</td>\n",
       "      <td>0.075246</td>\n",
       "      <td>-1.663383</td>\n",
       "      <td>0.859309</td>\n",
       "      <td>-0.262893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.166035</td>\n",
       "      <td>-0.181478</td>\n",
       "      <td>1.634437</td>\n",
       "      <td>0.904620</td>\n",
       "      <td>-0.424192</td>\n",
       "      <td>-0.452936</td>\n",
       "      <td>1.123414</td>\n",
       "      <td>0.534554</td>\n",
       "      <td>-0.380039</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.489450</td>\n",
       "      <td>-1.448590</td>\n",
       "      <td>2.194570</td>\n",
       "      <td>1.262672</td>\n",
       "      <td>0.504028</td>\n",
       "      <td>0.395338</td>\n",
       "      <td>-1.072729</td>\n",
       "      <td>0.879836</td>\n",
       "      <td>-1.542415</td>\n",
       "      <td>-0.938513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.231095</td>\n",
       "      <td>-0.209571</td>\n",
       "      <td>1.654951</td>\n",
       "      <td>1.247081</td>\n",
       "      <td>-0.652624</td>\n",
       "      <td>-0.546311</td>\n",
       "      <td>1.214370</td>\n",
       "      <td>0.821624</td>\n",
       "      <td>-0.502463</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.353433</td>\n",
       "      <td>-1.059878</td>\n",
       "      <td>2.589134</td>\n",
       "      <td>2.139926</td>\n",
       "      <td>0.593317</td>\n",
       "      <td>0.682023</td>\n",
       "      <td>-0.643181</td>\n",
       "      <td>-0.217332</td>\n",
       "      <td>0.945816</td>\n",
       "      <td>0.145041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.236090</td>\n",
       "      <td>-0.323013</td>\n",
       "      <td>-0.478244</td>\n",
       "      <td>-1.080788</td>\n",
       "      <td>-0.670161</td>\n",
       "      <td>-0.923364</td>\n",
       "      <td>-0.421866</td>\n",
       "      <td>-0.775114</td>\n",
       "      <td>-0.511862</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.443846</td>\n",
       "      <td>-0.627980</td>\n",
       "      <td>2.326862</td>\n",
       "      <td>3.114644</td>\n",
       "      <td>0.533965</td>\n",
       "      <td>1.000560</td>\n",
       "      <td>-0.323098</td>\n",
       "      <td>0.712401</td>\n",
       "      <td>-1.473404</td>\n",
       "      <td>-0.642872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5820 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  EDA_TonicMean_version02  EDA_TonicMean_version03  \\\n",
       "0   5                -0.123031                -0.226077   \n",
       "1   5                -0.152896                -0.050866   \n",
       "2   5                -0.166035                -0.181478   \n",
       "3   5                -0.231095                -0.209571   \n",
       "4   5                -0.236090                -0.323013   \n",
       "\n",
       "   EDA_TonicMean_version04  EDA_TonicMean_version05  EDA_TonicMean_version09  \\\n",
       "0                -1.220480                -1.697738                -0.273200   \n",
       "1                 1.527067                 1.883468                -0.378060   \n",
       "2                 1.634437                 0.904620                -0.424192   \n",
       "3                 1.654951                 1.247081                -0.652624   \n",
       "4                -0.478244                -1.080788                -0.670161   \n",
       "\n",
       "   EDA_TonicMean_version10  EDA_TonicMean_version11  EDA_TonicMean_version12  \\\n",
       "0                -0.601171                -0.809518                -1.012558   \n",
       "1                -0.018812                 1.023216                 1.189124   \n",
       "2                -0.452936                 1.123414                 0.534554   \n",
       "3                -0.546311                 1.214370                 0.821624   \n",
       "4                -0.923364                -0.421866                -0.775114   \n",
       "\n",
       "   EDA_TonicMean_version16  ...  EEG_avgRelTheta_version16  \\\n",
       "0                -0.299118  ...                  -1.877017   \n",
       "1                -0.355315  ...                  -1.632698   \n",
       "2                -0.380039  ...                  -1.489450   \n",
       "3                -0.502463  ...                  -1.353433   \n",
       "4                -0.511862  ...                  -1.443846   \n",
       "\n",
       "   EEG_avgRelTheta_version17  EEG_avgRelTheta_version19  \\\n",
       "0                  -1.442056                   1.070298   \n",
       "1                  -1.531970                   1.779032   \n",
       "2                  -1.448590                   2.194570   \n",
       "3                  -1.059878                   2.589134   \n",
       "4                  -0.627980                   2.326862   \n",
       "\n",
       "   EEG_avgRelTheta_version20  EEG_avgRelTheta_version22  \\\n",
       "0                   1.277417                   0.249605   \n",
       "1                   1.074498                   0.409991   \n",
       "2                   1.262672                   0.504028   \n",
       "3                   2.139926                   0.593317   \n",
       "4                   3.114644                   0.533965   \n",
       "\n",
       "   EEG_avgRelTheta_version23    adjSA1    adjSA2    adjSA3  adjSAtotal  \n",
       "0                   0.400156  0.119790  1.593122 -0.800726    0.350233  \n",
       "1                   0.333842  0.075246 -1.663383  0.859309   -0.262893  \n",
       "2                   0.395338 -1.072729  0.879836 -1.542415   -0.938513  \n",
       "3                   0.682023 -0.643181 -0.217332  0.945816    0.145041  \n",
       "4                   1.000560 -0.323098  0.712401 -1.473404   -0.642872  \n",
       "\n",
       "[5 rows x 5820 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv(\"./kieranFeatures_1-30_26-Sep-2024.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary variables for high and low \n",
    "adj_SA_1_median = np.median(df[\"adjSA1\"])\n",
    "adj_SA_2_median = np.median(df[\"adjSA2\"])\n",
    "adj_SA_3_median = np.median(df[\"adjSA3\"])\n",
    "adj_SA_tot_median = np.median(df[\"adjSAtotal\"])\n",
    "\n",
    "# Will be high if adjusted SA level score is equal to or above median, low otherwise\n",
    "df[\"Lv_1_Hi\"] = (df[\"adjSA1\"] >= adj_SA_1_median).astype(int)\n",
    "df[\"Lv_2_Hi\"] = (df[\"adjSA2\"] >= adj_SA_2_median).astype(int)\n",
    "df[\"Lv_3_Hi\"] = (df[\"adjSA3\"] >= adj_SA_3_median).astype(int)\n",
    "df[\"Tot_Hi\"] = (df[\"adjSAtotal\"] >= adj_SA_tot_median).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide up dataframe into predictors and outcomes. Train-test-split the following data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EDA_TonicMean_version02</th>\n",
       "      <th>EDA_TonicMean_version03</th>\n",
       "      <th>EDA_TonicMean_version04</th>\n",
       "      <th>EDA_TonicMean_version05</th>\n",
       "      <th>EDA_TonicMean_version09</th>\n",
       "      <th>EDA_TonicMean_version10</th>\n",
       "      <th>EDA_TonicMean_version11</th>\n",
       "      <th>EDA_TonicMean_version12</th>\n",
       "      <th>EDA_TonicMean_version16</th>\n",
       "      <th>EDA_TonicMean_version17</th>\n",
       "      <th>...</th>\n",
       "      <th>EEG_avgRelTheta_version09</th>\n",
       "      <th>EEG_avgRelTheta_version10</th>\n",
       "      <th>EEG_avgRelTheta_version11</th>\n",
       "      <th>EEG_avgRelTheta_version12</th>\n",
       "      <th>EEG_avgRelTheta_version16</th>\n",
       "      <th>EEG_avgRelTheta_version17</th>\n",
       "      <th>EEG_avgRelTheta_version19</th>\n",
       "      <th>EEG_avgRelTheta_version20</th>\n",
       "      <th>EEG_avgRelTheta_version22</th>\n",
       "      <th>EEG_avgRelTheta_version23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.123031</td>\n",
       "      <td>-0.226077</td>\n",
       "      <td>-1.220480</td>\n",
       "      <td>-1.697738</td>\n",
       "      <td>-0.273200</td>\n",
       "      <td>-0.601171</td>\n",
       "      <td>-0.809518</td>\n",
       "      <td>-1.012558</td>\n",
       "      <td>-0.299118</td>\n",
       "      <td>-0.469374</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.470055</td>\n",
       "      <td>-1.633813</td>\n",
       "      <td>-1.521523</td>\n",
       "      <td>-1.189742</td>\n",
       "      <td>-1.877017</td>\n",
       "      <td>-1.442056</td>\n",
       "      <td>1.070298</td>\n",
       "      <td>1.277417</td>\n",
       "      <td>0.249605</td>\n",
       "      <td>0.400156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.152896</td>\n",
       "      <td>-0.050866</td>\n",
       "      <td>1.527067</td>\n",
       "      <td>1.883468</td>\n",
       "      <td>-0.378060</td>\n",
       "      <td>-0.018812</td>\n",
       "      <td>1.023216</td>\n",
       "      <td>1.189124</td>\n",
       "      <td>-0.355315</td>\n",
       "      <td>-0.160570</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.999027</td>\n",
       "      <td>-1.796969</td>\n",
       "      <td>-0.890211</td>\n",
       "      <td>-0.846923</td>\n",
       "      <td>-1.632698</td>\n",
       "      <td>-1.531970</td>\n",
       "      <td>1.779032</td>\n",
       "      <td>1.074498</td>\n",
       "      <td>0.409991</td>\n",
       "      <td>0.333842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.166035</td>\n",
       "      <td>-0.181478</td>\n",
       "      <td>1.634437</td>\n",
       "      <td>0.904620</td>\n",
       "      <td>-0.424192</td>\n",
       "      <td>-0.452936</td>\n",
       "      <td>1.123414</td>\n",
       "      <td>0.534554</td>\n",
       "      <td>-0.380039</td>\n",
       "      <td>-0.390771</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.722859</td>\n",
       "      <td>-1.645669</td>\n",
       "      <td>-0.543299</td>\n",
       "      <td>-0.588502</td>\n",
       "      <td>-1.489450</td>\n",
       "      <td>-1.448590</td>\n",
       "      <td>2.194570</td>\n",
       "      <td>1.262672</td>\n",
       "      <td>0.504028</td>\n",
       "      <td>0.395338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.231095</td>\n",
       "      <td>-0.209571</td>\n",
       "      <td>1.654951</td>\n",
       "      <td>1.247081</td>\n",
       "      <td>-0.652624</td>\n",
       "      <td>-0.546311</td>\n",
       "      <td>1.214370</td>\n",
       "      <td>0.821624</td>\n",
       "      <td>-0.502463</td>\n",
       "      <td>-0.440284</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.460630</td>\n",
       "      <td>-0.940319</td>\n",
       "      <td>-0.955926</td>\n",
       "      <td>-0.744128</td>\n",
       "      <td>-1.353433</td>\n",
       "      <td>-1.059878</td>\n",
       "      <td>2.589134</td>\n",
       "      <td>2.139926</td>\n",
       "      <td>0.593317</td>\n",
       "      <td>0.682023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.236090</td>\n",
       "      <td>-0.323013</td>\n",
       "      <td>-0.478244</td>\n",
       "      <td>-1.080788</td>\n",
       "      <td>-0.670161</td>\n",
       "      <td>-0.923364</td>\n",
       "      <td>-0.421866</td>\n",
       "      <td>-0.775114</td>\n",
       "      <td>-0.511862</td>\n",
       "      <td>-0.640221</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.634937</td>\n",
       "      <td>-0.156605</td>\n",
       "      <td>-0.344389</td>\n",
       "      <td>0.214848</td>\n",
       "      <td>-1.443846</td>\n",
       "      <td>-0.627980</td>\n",
       "      <td>2.326862</td>\n",
       "      <td>3.114644</td>\n",
       "      <td>0.533965</td>\n",
       "      <td>1.000560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>-0.390463</td>\n",
       "      <td>-0.392143</td>\n",
       "      <td>-0.150550</td>\n",
       "      <td>-0.112208</td>\n",
       "      <td>-0.248912</td>\n",
       "      <td>-0.241573</td>\n",
       "      <td>-0.239724</td>\n",
       "      <td>-0.191818</td>\n",
       "      <td>-0.336535</td>\n",
       "      <td>-0.330997</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046622</td>\n",
       "      <td>-0.464843</td>\n",
       "      <td>0.523703</td>\n",
       "      <td>0.110708</td>\n",
       "      <td>0.163210</td>\n",
       "      <td>-0.308442</td>\n",
       "      <td>0.105745</td>\n",
       "      <td>-0.391989</td>\n",
       "      <td>0.798338</td>\n",
       "      <td>-0.432294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>-0.369596</td>\n",
       "      <td>-0.380586</td>\n",
       "      <td>-0.609280</td>\n",
       "      <td>-0.487820</td>\n",
       "      <td>-0.175647</td>\n",
       "      <td>-0.203160</td>\n",
       "      <td>-0.582746</td>\n",
       "      <td>-0.450147</td>\n",
       "      <td>-0.286580</td>\n",
       "      <td>-0.305082</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102853</td>\n",
       "      <td>-0.200243</td>\n",
       "      <td>0.539456</td>\n",
       "      <td>0.390624</td>\n",
       "      <td>0.101655</td>\n",
       "      <td>-0.000704</td>\n",
       "      <td>0.021136</td>\n",
       "      <td>-0.062902</td>\n",
       "      <td>0.571962</td>\n",
       "      <td>0.092100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>-0.457362</td>\n",
       "      <td>-0.382835</td>\n",
       "      <td>-1.247644</td>\n",
       "      <td>-0.132967</td>\n",
       "      <td>-0.483803</td>\n",
       "      <td>-0.210636</td>\n",
       "      <td>-1.131586</td>\n",
       "      <td>-0.204537</td>\n",
       "      <td>-0.496694</td>\n",
       "      <td>-0.310125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160880</td>\n",
       "      <td>-0.095837</td>\n",
       "      <td>2.240786</td>\n",
       "      <td>1.697116</td>\n",
       "      <td>0.390354</td>\n",
       "      <td>0.120724</td>\n",
       "      <td>0.417964</td>\n",
       "      <td>0.066950</td>\n",
       "      <td>1.633695</td>\n",
       "      <td>0.299016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>-0.370669</td>\n",
       "      <td>-0.390531</td>\n",
       "      <td>-1.078873</td>\n",
       "      <td>-0.866528</td>\n",
       "      <td>-0.179415</td>\n",
       "      <td>-0.236215</td>\n",
       "      <td>-0.917973</td>\n",
       "      <td>-0.699119</td>\n",
       "      <td>-0.289149</td>\n",
       "      <td>-0.327382</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.394919</td>\n",
       "      <td>-0.514674</td>\n",
       "      <td>-0.292277</td>\n",
       "      <td>-0.368327</td>\n",
       "      <td>-0.218058</td>\n",
       "      <td>-0.366396</td>\n",
       "      <td>-0.418322</td>\n",
       "      <td>-0.453964</td>\n",
       "      <td>-0.603831</td>\n",
       "      <td>-0.531050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>-0.375119</td>\n",
       "      <td>-0.381095</td>\n",
       "      <td>-0.045532</td>\n",
       "      <td>-0.083454</td>\n",
       "      <td>-0.195039</td>\n",
       "      <td>-0.204852</td>\n",
       "      <td>-0.151317</td>\n",
       "      <td>-0.168047</td>\n",
       "      <td>-0.299802</td>\n",
       "      <td>-0.306223</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.386655</td>\n",
       "      <td>0.364642</td>\n",
       "      <td>-0.872179</td>\n",
       "      <td>-0.296389</td>\n",
       "      <td>-0.209012</td>\n",
       "      <td>0.656275</td>\n",
       "      <td>-0.405888</td>\n",
       "      <td>0.639655</td>\n",
       "      <td>-0.570563</td>\n",
       "      <td>1.211611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>304 rows Ã— 5815 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     EDA_TonicMean_version02  EDA_TonicMean_version03  \\\n",
       "0                  -0.123031                -0.226077   \n",
       "1                  -0.152896                -0.050866   \n",
       "2                  -0.166035                -0.181478   \n",
       "3                  -0.231095                -0.209571   \n",
       "4                  -0.236090                -0.323013   \n",
       "..                       ...                      ...   \n",
       "299                -0.390463                -0.392143   \n",
       "300                -0.369596                -0.380586   \n",
       "301                -0.457362                -0.382835   \n",
       "302                -0.370669                -0.390531   \n",
       "303                -0.375119                -0.381095   \n",
       "\n",
       "     EDA_TonicMean_version04  EDA_TonicMean_version05  \\\n",
       "0                  -1.220480                -1.697738   \n",
       "1                   1.527067                 1.883468   \n",
       "2                   1.634437                 0.904620   \n",
       "3                   1.654951                 1.247081   \n",
       "4                  -0.478244                -1.080788   \n",
       "..                       ...                      ...   \n",
       "299                -0.150550                -0.112208   \n",
       "300                -0.609280                -0.487820   \n",
       "301                -1.247644                -0.132967   \n",
       "302                -1.078873                -0.866528   \n",
       "303                -0.045532                -0.083454   \n",
       "\n",
       "     EDA_TonicMean_version09  EDA_TonicMean_version10  \\\n",
       "0                  -0.273200                -0.601171   \n",
       "1                  -0.378060                -0.018812   \n",
       "2                  -0.424192                -0.452936   \n",
       "3                  -0.652624                -0.546311   \n",
       "4                  -0.670161                -0.923364   \n",
       "..                       ...                      ...   \n",
       "299                -0.248912                -0.241573   \n",
       "300                -0.175647                -0.203160   \n",
       "301                -0.483803                -0.210636   \n",
       "302                -0.179415                -0.236215   \n",
       "303                -0.195039                -0.204852   \n",
       "\n",
       "     EDA_TonicMean_version11  EDA_TonicMean_version12  \\\n",
       "0                  -0.809518                -1.012558   \n",
       "1                   1.023216                 1.189124   \n",
       "2                   1.123414                 0.534554   \n",
       "3                   1.214370                 0.821624   \n",
       "4                  -0.421866                -0.775114   \n",
       "..                       ...                      ...   \n",
       "299                -0.239724                -0.191818   \n",
       "300                -0.582746                -0.450147   \n",
       "301                -1.131586                -0.204537   \n",
       "302                -0.917973                -0.699119   \n",
       "303                -0.151317                -0.168047   \n",
       "\n",
       "     EDA_TonicMean_version16  EDA_TonicMean_version17  ...  \\\n",
       "0                  -0.299118                -0.469374  ...   \n",
       "1                  -0.355315                -0.160570  ...   \n",
       "2                  -0.380039                -0.390771  ...   \n",
       "3                  -0.502463                -0.440284  ...   \n",
       "4                  -0.511862                -0.640221  ...   \n",
       "..                       ...                      ...  ...   \n",
       "299                -0.336535                -0.330997  ...   \n",
       "300                -0.286580                -0.305082  ...   \n",
       "301                -0.496694                -0.310125  ...   \n",
       "302                -0.289149                -0.327382  ...   \n",
       "303                -0.299802                -0.306223  ...   \n",
       "\n",
       "     EEG_avgRelTheta_version09  EEG_avgRelTheta_version10  \\\n",
       "0                    -2.470055                  -1.633813   \n",
       "1                    -1.999027                  -1.796969   \n",
       "2                    -1.722859                  -1.645669   \n",
       "3                    -1.460630                  -0.940319   \n",
       "4                    -1.634937                  -0.156605   \n",
       "..                         ...                        ...   \n",
       "299                  -0.046622                  -0.464843   \n",
       "300                  -0.102853                  -0.200243   \n",
       "301                   0.160880                  -0.095837   \n",
       "302                  -0.394919                  -0.514674   \n",
       "303                  -0.386655                   0.364642   \n",
       "\n",
       "     EEG_avgRelTheta_version11  EEG_avgRelTheta_version12  \\\n",
       "0                    -1.521523                  -1.189742   \n",
       "1                    -0.890211                  -0.846923   \n",
       "2                    -0.543299                  -0.588502   \n",
       "3                    -0.955926                  -0.744128   \n",
       "4                    -0.344389                   0.214848   \n",
       "..                         ...                        ...   \n",
       "299                   0.523703                   0.110708   \n",
       "300                   0.539456                   0.390624   \n",
       "301                   2.240786                   1.697116   \n",
       "302                  -0.292277                  -0.368327   \n",
       "303                  -0.872179                  -0.296389   \n",
       "\n",
       "     EEG_avgRelTheta_version16  EEG_avgRelTheta_version17  \\\n",
       "0                    -1.877017                  -1.442056   \n",
       "1                    -1.632698                  -1.531970   \n",
       "2                    -1.489450                  -1.448590   \n",
       "3                    -1.353433                  -1.059878   \n",
       "4                    -1.443846                  -0.627980   \n",
       "..                         ...                        ...   \n",
       "299                   0.163210                  -0.308442   \n",
       "300                   0.101655                  -0.000704   \n",
       "301                   0.390354                   0.120724   \n",
       "302                  -0.218058                  -0.366396   \n",
       "303                  -0.209012                   0.656275   \n",
       "\n",
       "     EEG_avgRelTheta_version19  EEG_avgRelTheta_version20  \\\n",
       "0                     1.070298                   1.277417   \n",
       "1                     1.779032                   1.074498   \n",
       "2                     2.194570                   1.262672   \n",
       "3                     2.589134                   2.139926   \n",
       "4                     2.326862                   3.114644   \n",
       "..                         ...                        ...   \n",
       "299                   0.105745                  -0.391989   \n",
       "300                   0.021136                  -0.062902   \n",
       "301                   0.417964                   0.066950   \n",
       "302                  -0.418322                  -0.453964   \n",
       "303                  -0.405888                   0.639655   \n",
       "\n",
       "     EEG_avgRelTheta_version22  EEG_avgRelTheta_version23  \n",
       "0                     0.249605                   0.400156  \n",
       "1                     0.409991                   0.333842  \n",
       "2                     0.504028                   0.395338  \n",
       "3                     0.593317                   0.682023  \n",
       "4                     0.533965                   1.000560  \n",
       "..                         ...                        ...  \n",
       "299                   0.798338                  -0.432294  \n",
       "300                   0.571962                   0.092100  \n",
       "301                   1.633695                   0.299016  \n",
       "302                  -0.603831                  -0.531050  \n",
       "303                  -0.570563                   1.211611  \n",
       "\n",
       "[304 rows x 5815 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adjSA1</th>\n",
       "      <th>adjSA2</th>\n",
       "      <th>adjSA3</th>\n",
       "      <th>adjSAtotal</th>\n",
       "      <th>Lv_1_Hi</th>\n",
       "      <th>Lv_2_Hi</th>\n",
       "      <th>Lv_3_Hi</th>\n",
       "      <th>Tot_Hi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.119790</td>\n",
       "      <td>1.593122</td>\n",
       "      <td>-0.800726</td>\n",
       "      <td>0.350233</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.075246</td>\n",
       "      <td>-1.663383</td>\n",
       "      <td>0.859309</td>\n",
       "      <td>-0.262893</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.072729</td>\n",
       "      <td>0.879836</td>\n",
       "      <td>-1.542415</td>\n",
       "      <td>-0.938513</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.643181</td>\n",
       "      <td>-0.217332</td>\n",
       "      <td>0.945816</td>\n",
       "      <td>0.145041</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.323098</td>\n",
       "      <td>0.712401</td>\n",
       "      <td>-1.473404</td>\n",
       "      <td>-0.642872</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.076099</td>\n",
       "      <td>1.105227</td>\n",
       "      <td>-0.609431</td>\n",
       "      <td>0.209332</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>-0.258249</td>\n",
       "      <td>-0.360422</td>\n",
       "      <td>0.778641</td>\n",
       "      <td>0.155357</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0.110240</td>\n",
       "      <td>0.092504</td>\n",
       "      <td>0.945232</td>\n",
       "      <td>0.627581</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>-1.105639</td>\n",
       "      <td>0.426616</td>\n",
       "      <td>0.328063</td>\n",
       "      <td>-0.108335</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>1.141504</td>\n",
       "      <td>1.452440</td>\n",
       "      <td>0.883889</td>\n",
       "      <td>1.694167</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>304 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       adjSA1    adjSA2    adjSA3  adjSAtotal  Lv_1_Hi  Lv_2_Hi  Lv_3_Hi  \\\n",
       "0    0.119790  1.593122 -0.800726    0.350233        1        1        0   \n",
       "1    0.075246 -1.663383  0.859309   -0.262893        0        0        1   \n",
       "2   -1.072729  0.879836 -1.542415   -0.938513        0        1        0   \n",
       "3   -0.643181 -0.217332  0.945816    0.145041        0        0        1   \n",
       "4   -0.323098  0.712401 -1.473404   -0.642872        0        1        0   \n",
       "..        ...       ...       ...         ...      ...      ...      ...   \n",
       "299  0.076099  1.105227 -0.609431    0.209332        0        1        0   \n",
       "300 -0.258249 -0.360422  0.778641    0.155357        0        0        1   \n",
       "301  0.110240  0.092504  0.945232    0.627581        1        1        1   \n",
       "302 -1.105639  0.426616  0.328063   -0.108335        0        1        1   \n",
       "303  1.141504  1.452440  0.883889    1.694167        1        1        1   \n",
       "\n",
       "     Tot_Hi  \n",
       "0         1  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "..      ...  \n",
       "299       1  \n",
       "300       1  \n",
       "301       1  \n",
       "302       0  \n",
       "303       1  \n",
       "\n",
       "[304 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EDA_TonicMean_version02</th>\n",
       "      <th>EDA_TonicMean_version03</th>\n",
       "      <th>EDA_TonicMean_version04</th>\n",
       "      <th>EDA_TonicMean_version05</th>\n",
       "      <th>EDA_TonicMean_version09</th>\n",
       "      <th>EDA_TonicMean_version10</th>\n",
       "      <th>EDA_TonicMean_version11</th>\n",
       "      <th>EDA_TonicMean_version12</th>\n",
       "      <th>EDA_TonicMean_version16</th>\n",
       "      <th>EDA_TonicMean_version17</th>\n",
       "      <th>...</th>\n",
       "      <th>EEG_avgRelTheta_version09</th>\n",
       "      <th>EEG_avgRelTheta_version10</th>\n",
       "      <th>EEG_avgRelTheta_version11</th>\n",
       "      <th>EEG_avgRelTheta_version12</th>\n",
       "      <th>EEG_avgRelTheta_version16</th>\n",
       "      <th>EEG_avgRelTheta_version17</th>\n",
       "      <th>EEG_avgRelTheta_version19</th>\n",
       "      <th>EEG_avgRelTheta_version20</th>\n",
       "      <th>EEG_avgRelTheta_version22</th>\n",
       "      <th>EEG_avgRelTheta_version23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>-0.494618</td>\n",
       "      <td>-0.555541</td>\n",
       "      <td>-2.076390</td>\n",
       "      <td>-1.869410</td>\n",
       "      <td>0.597167</td>\n",
       "      <td>0.362062</td>\n",
       "      <td>-1.765695</td>\n",
       "      <td>-1.479098</td>\n",
       "      <td>0.418093</td>\n",
       "      <td>0.161042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.462172</td>\n",
       "      <td>0.859784</td>\n",
       "      <td>-0.051114</td>\n",
       "      <td>0.233826</td>\n",
       "      <td>0.237753</td>\n",
       "      <td>0.649436</td>\n",
       "      <td>0.111443</td>\n",
       "      <td>0.465456</td>\n",
       "      <td>0.779651</td>\n",
       "      <td>0.627392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>-0.495506</td>\n",
       "      <td>-0.500435</td>\n",
       "      <td>-0.131699</td>\n",
       "      <td>-0.129467</td>\n",
       "      <td>-0.662698</td>\n",
       "      <td>-0.644065</td>\n",
       "      <td>-0.258565</td>\n",
       "      <td>-0.237457</td>\n",
       "      <td>-0.612173</td>\n",
       "      <td>-0.596055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.260123</td>\n",
       "      <td>1.010109</td>\n",
       "      <td>-0.213657</td>\n",
       "      <td>0.601153</td>\n",
       "      <td>-0.307295</td>\n",
       "      <td>0.907735</td>\n",
       "      <td>-0.047399</td>\n",
       "      <td>1.014504</td>\n",
       "      <td>0.237016</td>\n",
       "      <td>1.841947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.297909</td>\n",
       "      <td>0.310984</td>\n",
       "      <td>-1.254804</td>\n",
       "      <td>-0.683124</td>\n",
       "      <td>-0.014281</td>\n",
       "      <td>0.030278</td>\n",
       "      <td>-0.612785</td>\n",
       "      <td>-0.323310</td>\n",
       "      <td>-0.147675</td>\n",
       "      <td>-0.124319</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.458536</td>\n",
       "      <td>-2.291505</td>\n",
       "      <td>-0.823308</td>\n",
       "      <td>-1.187300</td>\n",
       "      <td>-1.355405</td>\n",
       "      <td>-1.868532</td>\n",
       "      <td>-0.703187</td>\n",
       "      <td>-1.310684</td>\n",
       "      <td>-0.785986</td>\n",
       "      <td>-1.462410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>-0.751382</td>\n",
       "      <td>-0.767114</td>\n",
       "      <td>0.220138</td>\n",
       "      <td>-0.002063</td>\n",
       "      <td>-1.119621</td>\n",
       "      <td>-1.112651</td>\n",
       "      <td>0.082926</td>\n",
       "      <td>-0.217642</td>\n",
       "      <td>-1.034901</td>\n",
       "      <td>-1.023674</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084950</td>\n",
       "      <td>-0.236394</td>\n",
       "      <td>-1.212890</td>\n",
       "      <td>-1.107865</td>\n",
       "      <td>-0.042654</td>\n",
       "      <td>-0.178245</td>\n",
       "      <td>-0.830580</td>\n",
       "      <td>-0.678148</td>\n",
       "      <td>-1.206773</td>\n",
       "      <td>-0.626839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>-0.354384</td>\n",
       "      <td>-0.332566</td>\n",
       "      <td>-1.195093</td>\n",
       "      <td>-0.569050</td>\n",
       "      <td>-1.246429</td>\n",
       "      <td>-1.107403</td>\n",
       "      <td>-0.981167</td>\n",
       "      <td>-0.478398</td>\n",
       "      <td>-0.795857</td>\n",
       "      <td>-0.715333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509455</td>\n",
       "      <td>0.654922</td>\n",
       "      <td>0.548907</td>\n",
       "      <td>0.656188</td>\n",
       "      <td>0.507635</td>\n",
       "      <td>0.712548</td>\n",
       "      <td>-0.440298</td>\n",
       "      <td>0.200804</td>\n",
       "      <td>-0.601341</td>\n",
       "      <td>0.526651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>-0.297054</td>\n",
       "      <td>-0.307219</td>\n",
       "      <td>-0.661230</td>\n",
       "      <td>-0.513394</td>\n",
       "      <td>-1.014946</td>\n",
       "      <td>-0.994582</td>\n",
       "      <td>-0.575531</td>\n",
       "      <td>-0.431628</td>\n",
       "      <td>-0.679828</td>\n",
       "      <td>-0.661295</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.726769</td>\n",
       "      <td>-1.195044</td>\n",
       "      <td>-0.629046</td>\n",
       "      <td>-0.860808</td>\n",
       "      <td>-0.798998</td>\n",
       "      <td>-1.181357</td>\n",
       "      <td>0.012439</td>\n",
       "      <td>-0.598611</td>\n",
       "      <td>0.566104</td>\n",
       "      <td>-0.681974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>-0.559570</td>\n",
       "      <td>-0.562361</td>\n",
       "      <td>1.381119</td>\n",
       "      <td>0.854606</td>\n",
       "      <td>-0.446144</td>\n",
       "      <td>-0.432101</td>\n",
       "      <td>1.443925</td>\n",
       "      <td>0.759285</td>\n",
       "      <td>-0.517737</td>\n",
       "      <td>-0.506605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.597457</td>\n",
       "      <td>0.281063</td>\n",
       "      <td>-0.908693</td>\n",
       "      <td>-0.867127</td>\n",
       "      <td>0.619710</td>\n",
       "      <td>0.355382</td>\n",
       "      <td>0.196207</td>\n",
       "      <td>-0.034579</td>\n",
       "      <td>0.716476</td>\n",
       "      <td>0.070465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>-0.362930</td>\n",
       "      <td>-0.376268</td>\n",
       "      <td>-0.392341</td>\n",
       "      <td>-0.370327</td>\n",
       "      <td>-0.500164</td>\n",
       "      <td>-0.518055</td>\n",
       "      <td>-0.417380</td>\n",
       "      <td>-0.369009</td>\n",
       "      <td>-0.471410</td>\n",
       "      <td>-0.478937</td>\n",
       "      <td>...</td>\n",
       "      <td>1.048729</td>\n",
       "      <td>1.497494</td>\n",
       "      <td>-0.488310</td>\n",
       "      <td>-0.037478</td>\n",
       "      <td>1.458071</td>\n",
       "      <td>2.088352</td>\n",
       "      <td>-0.736493</td>\n",
       "      <td>0.362864</td>\n",
       "      <td>-1.311561</td>\n",
       "      <td>0.741032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>-0.663595</td>\n",
       "      <td>-0.680655</td>\n",
       "      <td>-0.323792</td>\n",
       "      <td>-0.360686</td>\n",
       "      <td>0.003870</td>\n",
       "      <td>-0.053789</td>\n",
       "      <td>-0.548041</td>\n",
       "      <td>-0.547836</td>\n",
       "      <td>-0.197162</td>\n",
       "      <td>-0.265638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.846641</td>\n",
       "      <td>1.038962</td>\n",
       "      <td>0.487292</td>\n",
       "      <td>0.634474</td>\n",
       "      <td>0.559359</td>\n",
       "      <td>0.808679</td>\n",
       "      <td>0.689936</td>\n",
       "      <td>0.688303</td>\n",
       "      <td>2.265877</td>\n",
       "      <td>0.874194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-0.219538</td>\n",
       "      <td>-0.209273</td>\n",
       "      <td>-0.103224</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.036997</td>\n",
       "      <td>-0.163482</td>\n",
       "      <td>-0.066556</td>\n",
       "      <td>-0.158862</td>\n",
       "      <td>-0.138007</td>\n",
       "      <td>...</td>\n",
       "      <td>1.671283</td>\n",
       "      <td>1.827563</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>1.212934</td>\n",
       "      <td>2.163855</td>\n",
       "      <td>2.485919</td>\n",
       "      <td>0.200236</td>\n",
       "      <td>0.773377</td>\n",
       "      <td>0.940547</td>\n",
       "      <td>1.372242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243 rows Ã— 5815 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     EDA_TonicMean_version02  EDA_TonicMean_version03  \\\n",
       "269                -0.494618                -0.555541   \n",
       "211                -0.495506                -0.500435   \n",
       "197                 0.297909                 0.310984   \n",
       "75                 -0.751382                -0.767114   \n",
       "177                -0.354384                -0.332566   \n",
       "..                       ...                      ...   \n",
       "188                -0.297054                -0.307219   \n",
       "71                 -0.559570                -0.562361   \n",
       "106                -0.362930                -0.376268   \n",
       "270                -0.663595                -0.680655   \n",
       "102                -0.219538                -0.209273   \n",
       "\n",
       "     EDA_TonicMean_version04  EDA_TonicMean_version05  \\\n",
       "269                -2.076390                -1.869410   \n",
       "211                -0.131699                -0.129467   \n",
       "197                -1.254804                -0.683124   \n",
       "75                  0.220138                -0.002063   \n",
       "177                -1.195093                -0.569050   \n",
       "..                       ...                      ...   \n",
       "188                -0.661230                -0.513394   \n",
       "71                  1.381119                 0.854606   \n",
       "106                -0.392341                -0.370327   \n",
       "270                -0.323792                -0.360686   \n",
       "102                -0.103224                 0.025001   \n",
       "\n",
       "     EDA_TonicMean_version09  EDA_TonicMean_version10  \\\n",
       "269                 0.597167                 0.362062   \n",
       "211                -0.662698                -0.644065   \n",
       "197                -0.014281                 0.030278   \n",
       "75                 -1.119621                -1.112651   \n",
       "177                -1.246429                -1.107403   \n",
       "..                       ...                      ...   \n",
       "188                -1.014946                -0.994582   \n",
       "71                 -0.446144                -0.432101   \n",
       "106                -0.500164                -0.518055   \n",
       "270                 0.003870                -0.053789   \n",
       "102                 0.003300                 0.036997   \n",
       "\n",
       "     EDA_TonicMean_version11  EDA_TonicMean_version12  \\\n",
       "269                -1.765695                -1.479098   \n",
       "211                -0.258565                -0.237457   \n",
       "197                -0.612785                -0.323310   \n",
       "75                  0.082926                -0.217642   \n",
       "177                -0.981167                -0.478398   \n",
       "..                       ...                      ...   \n",
       "188                -0.575531                -0.431628   \n",
       "71                  1.443925                 0.759285   \n",
       "106                -0.417380                -0.369009   \n",
       "270                -0.548041                -0.547836   \n",
       "102                -0.163482                -0.066556   \n",
       "\n",
       "     EDA_TonicMean_version16  EDA_TonicMean_version17  ...  \\\n",
       "269                 0.418093                 0.161042  ...   \n",
       "211                -0.612173                -0.596055  ...   \n",
       "197                -0.147675                -0.124319  ...   \n",
       "75                 -1.034901                -1.023674  ...   \n",
       "177                -0.795857                -0.715333  ...   \n",
       "..                       ...                      ...  ...   \n",
       "188                -0.679828                -0.661295  ...   \n",
       "71                 -0.517737                -0.506605  ...   \n",
       "106                -0.471410                -0.478937  ...   \n",
       "270                -0.197162                -0.265638  ...   \n",
       "102                -0.158862                -0.138007  ...   \n",
       "\n",
       "     EEG_avgRelTheta_version09  EEG_avgRelTheta_version10  \\\n",
       "269                   0.462172                   0.859784   \n",
       "211                  -0.260123                   1.010109   \n",
       "197                  -1.458536                  -2.291505   \n",
       "75                   -0.084950                  -0.236394   \n",
       "177                   0.509455                   0.654922   \n",
       "..                         ...                        ...   \n",
       "188                  -0.726769                  -1.195044   \n",
       "71                    0.597457                   0.281063   \n",
       "106                   1.048729                   1.497494   \n",
       "270                   0.846641                   1.038962   \n",
       "102                   1.671283                   1.827563   \n",
       "\n",
       "     EEG_avgRelTheta_version11  EEG_avgRelTheta_version12  \\\n",
       "269                  -0.051114                   0.233826   \n",
       "211                  -0.213657                   0.601153   \n",
       "197                  -0.823308                  -1.187300   \n",
       "75                   -1.212890                  -1.107865   \n",
       "177                   0.548907                   0.656188   \n",
       "..                         ...                        ...   \n",
       "188                  -0.629046                  -0.860808   \n",
       "71                   -0.908693                  -0.867127   \n",
       "106                  -0.488310                  -0.037478   \n",
       "270                   0.487292                   0.634474   \n",
       "102                   0.991150                   1.212934   \n",
       "\n",
       "     EEG_avgRelTheta_version16  EEG_avgRelTheta_version17  \\\n",
       "269                   0.237753                   0.649436   \n",
       "211                  -0.307295                   0.907735   \n",
       "197                  -1.355405                  -1.868532   \n",
       "75                   -0.042654                  -0.178245   \n",
       "177                   0.507635                   0.712548   \n",
       "..                         ...                        ...   \n",
       "188                  -0.798998                  -1.181357   \n",
       "71                    0.619710                   0.355382   \n",
       "106                   1.458071                   2.088352   \n",
       "270                   0.559359                   0.808679   \n",
       "102                   2.163855                   2.485919   \n",
       "\n",
       "     EEG_avgRelTheta_version19  EEG_avgRelTheta_version20  \\\n",
       "269                   0.111443                   0.465456   \n",
       "211                  -0.047399                   1.014504   \n",
       "197                  -0.703187                  -1.310684   \n",
       "75                   -0.830580                  -0.678148   \n",
       "177                  -0.440298                   0.200804   \n",
       "..                         ...                        ...   \n",
       "188                   0.012439                  -0.598611   \n",
       "71                    0.196207                  -0.034579   \n",
       "106                  -0.736493                   0.362864   \n",
       "270                   0.689936                   0.688303   \n",
       "102                   0.200236                   0.773377   \n",
       "\n",
       "     EEG_avgRelTheta_version22  EEG_avgRelTheta_version23  \n",
       "269                   0.779651                   0.627392  \n",
       "211                   0.237016                   1.841947  \n",
       "197                  -0.785986                  -1.462410  \n",
       "75                   -1.206773                  -0.626839  \n",
       "177                  -0.601341                   0.526651  \n",
       "..                         ...                        ...  \n",
       "188                   0.566104                  -0.681974  \n",
       "71                    0.716476                   0.070465  \n",
       "106                  -1.311561                   0.741032  \n",
       "270                   2.265877                   0.874194  \n",
       "102                   0.940547                   1.372242  \n",
       "\n",
       "[243 rows x 5815 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adjSA1</th>\n",
       "      <th>adjSA2</th>\n",
       "      <th>adjSA3</th>\n",
       "      <th>adjSAtotal</th>\n",
       "      <th>Lv_1_Hi</th>\n",
       "      <th>Lv_2_Hi</th>\n",
       "      <th>Lv_3_Hi</th>\n",
       "      <th>Tot_Hi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>0.143670</td>\n",
       "      <td>1.101324</td>\n",
       "      <td>-0.822506</td>\n",
       "      <td>0.117187</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>-0.783965</td>\n",
       "      <td>-0.834554</td>\n",
       "      <td>0.116317</td>\n",
       "      <td>-0.677632</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.475912</td>\n",
       "      <td>-0.162414</td>\n",
       "      <td>1.180378</td>\n",
       "      <td>0.804260</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>-0.088596</td>\n",
       "      <td>0.229732</td>\n",
       "      <td>0.972713</td>\n",
       "      <td>0.618738</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>-1.146056</td>\n",
       "      <td>0.662741</td>\n",
       "      <td>-3.060939</td>\n",
       "      <td>-1.932524</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1.116062</td>\n",
       "      <td>0.313836</td>\n",
       "      <td>0.270247</td>\n",
       "      <td>0.799881</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>-0.881938</td>\n",
       "      <td>0.311514</td>\n",
       "      <td>-0.739256</td>\n",
       "      <td>-0.666210</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>-0.900207</td>\n",
       "      <td>0.372292</td>\n",
       "      <td>-1.423042</td>\n",
       "      <td>-1.032613</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>-2.025591</td>\n",
       "      <td>0.753425</td>\n",
       "      <td>0.139548</td>\n",
       "      <td>-0.472791</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-0.002411</td>\n",
       "      <td>-1.686081</td>\n",
       "      <td>0.460911</td>\n",
       "      <td>-0.533697</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       adjSA1    adjSA2    adjSA3  adjSAtotal  Lv_1_Hi  Lv_2_Hi  Lv_3_Hi  \\\n",
       "269  0.143670  1.101324 -0.822506    0.117187        1        1        0   \n",
       "211 -0.783965 -0.834554  0.116317   -0.677632        0        0        0   \n",
       "197  0.475912 -0.162414  1.180378    0.804260        1        0        1   \n",
       "75  -0.088596  0.229732  0.972713    0.618738        0        1        1   \n",
       "177 -1.146056  0.662741 -3.060939   -1.932524        0        1        0   \n",
       "..        ...       ...       ...         ...      ...      ...      ...   \n",
       "188  1.116062  0.313836  0.270247    0.799881        1        1        1   \n",
       "71  -0.881938  0.311514 -0.739256   -0.666210        0        1        0   \n",
       "106 -0.900207  0.372292 -1.423042   -1.032613        0        1        0   \n",
       "270 -2.025591  0.753425  0.139548   -0.472791        0        1        0   \n",
       "102 -0.002411 -1.686081  0.460911   -0.533697        0        0        1   \n",
       "\n",
       "     Tot_Hi  \n",
       "269       0  \n",
       "211       0  \n",
       "197       1  \n",
       "75        1  \n",
       "177       0  \n",
       "..      ...  \n",
       "188       1  \n",
       "71        0  \n",
       "106       0  \n",
       "270       0  \n",
       "102       0  \n",
       "\n",
       "[243 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EDA_TonicMean_version02</th>\n",
       "      <th>EDA_TonicMean_version03</th>\n",
       "      <th>EDA_TonicMean_version04</th>\n",
       "      <th>EDA_TonicMean_version05</th>\n",
       "      <th>EDA_TonicMean_version09</th>\n",
       "      <th>EDA_TonicMean_version10</th>\n",
       "      <th>EDA_TonicMean_version11</th>\n",
       "      <th>EDA_TonicMean_version12</th>\n",
       "      <th>EDA_TonicMean_version16</th>\n",
       "      <th>EDA_TonicMean_version17</th>\n",
       "      <th>...</th>\n",
       "      <th>EEG_avgRelTheta_version09</th>\n",
       "      <th>EEG_avgRelTheta_version10</th>\n",
       "      <th>EEG_avgRelTheta_version11</th>\n",
       "      <th>EEG_avgRelTheta_version12</th>\n",
       "      <th>EEG_avgRelTheta_version16</th>\n",
       "      <th>EEG_avgRelTheta_version17</th>\n",
       "      <th>EEG_avgRelTheta_version19</th>\n",
       "      <th>EEG_avgRelTheta_version20</th>\n",
       "      <th>EEG_avgRelTheta_version22</th>\n",
       "      <th>EEG_avgRelTheta_version23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.498774</td>\n",
       "      <td>-0.560976</td>\n",
       "      <td>-2.050127</td>\n",
       "      <td>-1.854451</td>\n",
       "      <td>0.577210</td>\n",
       "      <td>0.339142</td>\n",
       "      <td>-1.697864</td>\n",
       "      <td>-1.405132</td>\n",
       "      <td>0.394787</td>\n",
       "      <td>0.143846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454675</td>\n",
       "      <td>0.827464</td>\n",
       "      <td>-0.030701</td>\n",
       "      <td>0.300150</td>\n",
       "      <td>0.224543</td>\n",
       "      <td>0.608457</td>\n",
       "      <td>0.108515</td>\n",
       "      <td>0.414417</td>\n",
       "      <td>0.830405</td>\n",
       "      <td>0.605829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.499679</td>\n",
       "      <td>-0.504911</td>\n",
       "      <td>-0.144123</td>\n",
       "      <td>-0.145626</td>\n",
       "      <td>-0.690567</td>\n",
       "      <td>-0.675169</td>\n",
       "      <td>-0.258050</td>\n",
       "      <td>-0.237600</td>\n",
       "      <td>-0.613827</td>\n",
       "      <td>-0.600710</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.263765</td>\n",
       "      <td>0.979137</td>\n",
       "      <td>-0.216744</td>\n",
       "      <td>0.755464</td>\n",
       "      <td>-0.320508</td>\n",
       "      <td>0.866908</td>\n",
       "      <td>-0.045040</td>\n",
       "      <td>0.958251</td>\n",
       "      <td>0.276126</td>\n",
       "      <td>1.835104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.308872</td>\n",
       "      <td>0.320641</td>\n",
       "      <td>-1.244885</td>\n",
       "      <td>-0.689381</td>\n",
       "      <td>-0.038077</td>\n",
       "      <td>0.004659</td>\n",
       "      <td>-0.596449</td>\n",
       "      <td>-0.318328</td>\n",
       "      <td>-0.159091</td>\n",
       "      <td>-0.136788</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.455783</td>\n",
       "      <td>-2.352085</td>\n",
       "      <td>-0.914538</td>\n",
       "      <td>-1.461383</td>\n",
       "      <td>-1.368624</td>\n",
       "      <td>-1.910998</td>\n",
       "      <td>-0.678999</td>\n",
       "      <td>-1.344857</td>\n",
       "      <td>-0.768828</td>\n",
       "      <td>-1.509301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.760436</td>\n",
       "      <td>-0.776234</td>\n",
       "      <td>0.200714</td>\n",
       "      <td>-0.020500</td>\n",
       "      <td>-1.150359</td>\n",
       "      <td>-1.147566</td>\n",
       "      <td>0.068188</td>\n",
       "      <td>-0.218968</td>\n",
       "      <td>-1.027671</td>\n",
       "      <td>-1.021246</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089527</td>\n",
       "      <td>-0.278544</td>\n",
       "      <td>-1.360446</td>\n",
       "      <td>-1.362921</td>\n",
       "      <td>-0.055866</td>\n",
       "      <td>-0.219713</td>\n",
       "      <td>-0.802152</td>\n",
       "      <td>-0.718327</td>\n",
       "      <td>-1.198644</td>\n",
       "      <td>-0.663603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.355864</td>\n",
       "      <td>-0.334118</td>\n",
       "      <td>-1.186363</td>\n",
       "      <td>-0.577347</td>\n",
       "      <td>-1.277963</td>\n",
       "      <td>-1.142275</td>\n",
       "      <td>-0.948377</td>\n",
       "      <td>-0.464160</td>\n",
       "      <td>-0.793650</td>\n",
       "      <td>-0.718012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501706</td>\n",
       "      <td>0.620765</td>\n",
       "      <td>0.656070</td>\n",
       "      <td>0.823681</td>\n",
       "      <td>0.494427</td>\n",
       "      <td>0.671606</td>\n",
       "      <td>-0.424861</td>\n",
       "      <td>0.152278</td>\n",
       "      <td>-0.580220</td>\n",
       "      <td>0.503866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>-0.297441</td>\n",
       "      <td>-0.308329</td>\n",
       "      <td>-0.663120</td>\n",
       "      <td>-0.522686</td>\n",
       "      <td>-1.045026</td>\n",
       "      <td>-1.028537</td>\n",
       "      <td>-0.560859</td>\n",
       "      <td>-0.420182</td>\n",
       "      <td>-0.680061</td>\n",
       "      <td>-0.664869</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.727921</td>\n",
       "      <td>-1.245791</td>\n",
       "      <td>-0.692190</td>\n",
       "      <td>-1.056685</td>\n",
       "      <td>-0.812214</td>\n",
       "      <td>-1.223418</td>\n",
       "      <td>0.012807</td>\n",
       "      <td>-0.639546</td>\n",
       "      <td>0.612276</td>\n",
       "      <td>-0.719407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>-0.564964</td>\n",
       "      <td>-0.567915</td>\n",
       "      <td>1.338598</td>\n",
       "      <td>0.820848</td>\n",
       "      <td>-0.472653</td>\n",
       "      <td>-0.461481</td>\n",
       "      <td>1.368398</td>\n",
       "      <td>0.699650</td>\n",
       "      <td>-0.521375</td>\n",
       "      <td>-0.512742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.589238</td>\n",
       "      <td>0.243553</td>\n",
       "      <td>-1.012268</td>\n",
       "      <td>-1.064518</td>\n",
       "      <td>0.606502</td>\n",
       "      <td>0.314229</td>\n",
       "      <td>0.190458</td>\n",
       "      <td>-0.080869</td>\n",
       "      <td>0.765875</td>\n",
       "      <td>0.042152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>-0.364573</td>\n",
       "      <td>-0.378581</td>\n",
       "      <td>-0.399581</td>\n",
       "      <td>-0.382179</td>\n",
       "      <td>-0.527012</td>\n",
       "      <td>-0.548134</td>\n",
       "      <td>-0.409772</td>\n",
       "      <td>-0.361300</td>\n",
       "      <td>-0.476023</td>\n",
       "      <td>-0.485532</td>\n",
       "      <td>...</td>\n",
       "      <td>1.038101</td>\n",
       "      <td>1.470893</td>\n",
       "      <td>-0.531106</td>\n",
       "      <td>-0.036140</td>\n",
       "      <td>1.444868</td>\n",
       "      <td>2.048222</td>\n",
       "      <td>-0.711197</td>\n",
       "      <td>0.312800</td>\n",
       "      <td>-1.305680</td>\n",
       "      <td>0.720846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>-0.670974</td>\n",
       "      <td>-0.688270</td>\n",
       "      <td>-0.332395</td>\n",
       "      <td>-0.372710</td>\n",
       "      <td>-0.019813</td>\n",
       "      <td>-0.080091</td>\n",
       "      <td>-0.534597</td>\n",
       "      <td>-0.529454</td>\n",
       "      <td>-0.207538</td>\n",
       "      <td>-0.275766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.837092</td>\n",
       "      <td>1.008249</td>\n",
       "      <td>0.585547</td>\n",
       "      <td>0.796767</td>\n",
       "      <td>0.546151</td>\n",
       "      <td>0.767793</td>\n",
       "      <td>0.667753</td>\n",
       "      <td>0.635148</td>\n",
       "      <td>2.348523</td>\n",
       "      <td>0.855622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>-0.218446</td>\n",
       "      <td>-0.208677</td>\n",
       "      <td>-0.116215</td>\n",
       "      <td>0.006080</td>\n",
       "      <td>-0.020386</td>\n",
       "      <td>0.011433</td>\n",
       "      <td>-0.167215</td>\n",
       "      <td>-0.076900</td>\n",
       "      <td>-0.170043</td>\n",
       "      <td>-0.150250</td>\n",
       "      <td>...</td>\n",
       "      <td>1.657333</td>\n",
       "      <td>1.803922</td>\n",
       "      <td>1.162252</td>\n",
       "      <td>1.513787</td>\n",
       "      <td>2.150656</td>\n",
       "      <td>2.446024</td>\n",
       "      <td>0.194353</td>\n",
       "      <td>0.719414</td>\n",
       "      <td>0.994753</td>\n",
       "      <td>1.359706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243 rows Ã— 5815 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     EDA_TonicMean_version02  EDA_TonicMean_version03  \\\n",
       "0                  -0.498774                -0.560976   \n",
       "1                  -0.499679                -0.504911   \n",
       "2                   0.308872                 0.320641   \n",
       "3                  -0.760436                -0.776234   \n",
       "4                  -0.355864                -0.334118   \n",
       "..                       ...                      ...   \n",
       "238                -0.297441                -0.308329   \n",
       "239                -0.564964                -0.567915   \n",
       "240                -0.364573                -0.378581   \n",
       "241                -0.670974                -0.688270   \n",
       "242                -0.218446                -0.208677   \n",
       "\n",
       "     EDA_TonicMean_version04  EDA_TonicMean_version05  \\\n",
       "0                  -2.050127                -1.854451   \n",
       "1                  -0.144123                -0.145626   \n",
       "2                  -1.244885                -0.689381   \n",
       "3                   0.200714                -0.020500   \n",
       "4                  -1.186363                -0.577347   \n",
       "..                       ...                      ...   \n",
       "238                -0.663120                -0.522686   \n",
       "239                 1.338598                 0.820848   \n",
       "240                -0.399581                -0.382179   \n",
       "241                -0.332395                -0.372710   \n",
       "242                -0.116215                 0.006080   \n",
       "\n",
       "     EDA_TonicMean_version09  EDA_TonicMean_version10  \\\n",
       "0                   0.577210                 0.339142   \n",
       "1                  -0.690567                -0.675169   \n",
       "2                  -0.038077                 0.004659   \n",
       "3                  -1.150359                -1.147566   \n",
       "4                  -1.277963                -1.142275   \n",
       "..                       ...                      ...   \n",
       "238                -1.045026                -1.028537   \n",
       "239                -0.472653                -0.461481   \n",
       "240                -0.527012                -0.548134   \n",
       "241                -0.019813                -0.080091   \n",
       "242                -0.020386                 0.011433   \n",
       "\n",
       "     EDA_TonicMean_version11  EDA_TonicMean_version12  \\\n",
       "0                  -1.697864                -1.405132   \n",
       "1                  -0.258050                -0.237600   \n",
       "2                  -0.596449                -0.318328   \n",
       "3                   0.068188                -0.218968   \n",
       "4                  -0.948377                -0.464160   \n",
       "..                       ...                      ...   \n",
       "238                -0.560859                -0.420182   \n",
       "239                 1.368398                 0.699650   \n",
       "240                -0.409772                -0.361300   \n",
       "241                -0.534597                -0.529454   \n",
       "242                -0.167215                -0.076900   \n",
       "\n",
       "     EDA_TonicMean_version16  EDA_TonicMean_version17  ...  \\\n",
       "0                   0.394787                 0.143846  ...   \n",
       "1                  -0.613827                -0.600710  ...   \n",
       "2                  -0.159091                -0.136788  ...   \n",
       "3                  -1.027671                -1.021246  ...   \n",
       "4                  -0.793650                -0.718012  ...   \n",
       "..                       ...                      ...  ...   \n",
       "238                -0.680061                -0.664869  ...   \n",
       "239                -0.521375                -0.512742  ...   \n",
       "240                -0.476023                -0.485532  ...   \n",
       "241                -0.207538                -0.275766  ...   \n",
       "242                -0.170043                -0.150250  ...   \n",
       "\n",
       "     EEG_avgRelTheta_version09  EEG_avgRelTheta_version10  \\\n",
       "0                     0.454675                   0.827464   \n",
       "1                    -0.263765                   0.979137   \n",
       "2                    -1.455783                  -2.352085   \n",
       "3                    -0.089527                  -0.278544   \n",
       "4                     0.501706                   0.620765   \n",
       "..                         ...                        ...   \n",
       "238                  -0.727921                  -1.245791   \n",
       "239                   0.589238                   0.243553   \n",
       "240                   1.038101                   1.470893   \n",
       "241                   0.837092                   1.008249   \n",
       "242                   1.657333                   1.803922   \n",
       "\n",
       "     EEG_avgRelTheta_version11  EEG_avgRelTheta_version12  \\\n",
       "0                    -0.030701                   0.300150   \n",
       "1                    -0.216744                   0.755464   \n",
       "2                    -0.914538                  -1.461383   \n",
       "3                    -1.360446                  -1.362921   \n",
       "4                     0.656070                   0.823681   \n",
       "..                         ...                        ...   \n",
       "238                  -0.692190                  -1.056685   \n",
       "239                  -1.012268                  -1.064518   \n",
       "240                  -0.531106                  -0.036140   \n",
       "241                   0.585547                   0.796767   \n",
       "242                   1.162252                   1.513787   \n",
       "\n",
       "     EEG_avgRelTheta_version16  EEG_avgRelTheta_version17  \\\n",
       "0                     0.224543                   0.608457   \n",
       "1                    -0.320508                   0.866908   \n",
       "2                    -1.368624                  -1.910998   \n",
       "3                    -0.055866                  -0.219713   \n",
       "4                     0.494427                   0.671606   \n",
       "..                         ...                        ...   \n",
       "238                  -0.812214                  -1.223418   \n",
       "239                   0.606502                   0.314229   \n",
       "240                   1.444868                   2.048222   \n",
       "241                   0.546151                   0.767793   \n",
       "242                   2.150656                   2.446024   \n",
       "\n",
       "     EEG_avgRelTheta_version19  EEG_avgRelTheta_version20  \\\n",
       "0                     0.108515                   0.414417   \n",
       "1                    -0.045040                   0.958251   \n",
       "2                    -0.678999                  -1.344857   \n",
       "3                    -0.802152                  -0.718327   \n",
       "4                    -0.424861                   0.152278   \n",
       "..                         ...                        ...   \n",
       "238                   0.012807                  -0.639546   \n",
       "239                   0.190458                  -0.080869   \n",
       "240                  -0.711197                   0.312800   \n",
       "241                   0.667753                   0.635148   \n",
       "242                   0.194353                   0.719414   \n",
       "\n",
       "     EEG_avgRelTheta_version22  EEG_avgRelTheta_version23  \n",
       "0                     0.830405                   0.605829  \n",
       "1                     0.276126                   1.835104  \n",
       "2                    -0.768828                  -1.509301  \n",
       "3                    -1.198644                  -0.663603  \n",
       "4                    -0.580220                   0.503866  \n",
       "..                         ...                        ...  \n",
       "238                   0.612276                  -0.719407  \n",
       "239                   0.765875                   0.042152  \n",
       "240                  -1.305680                   0.720846  \n",
       "241                   2.348523                   0.855622  \n",
       "242                   0.994753                   1.359706  \n",
       "\n",
       "[243 rows x 5815 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adjSA1</th>\n",
       "      <th>adjSA2</th>\n",
       "      <th>adjSA3</th>\n",
       "      <th>adjSAtotal</th>\n",
       "      <th>Lv_1_Hi</th>\n",
       "      <th>Lv_2_Hi</th>\n",
       "      <th>Lv_3_Hi</th>\n",
       "      <th>Tot_Hi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>0.143670</td>\n",
       "      <td>1.101324</td>\n",
       "      <td>-0.822506</td>\n",
       "      <td>0.117187</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>-0.783965</td>\n",
       "      <td>-0.834554</td>\n",
       "      <td>0.116317</td>\n",
       "      <td>-0.677632</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.475912</td>\n",
       "      <td>-0.162414</td>\n",
       "      <td>1.180378</td>\n",
       "      <td>0.804260</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>-0.088596</td>\n",
       "      <td>0.229732</td>\n",
       "      <td>0.972713</td>\n",
       "      <td>0.618738</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>-1.146056</td>\n",
       "      <td>0.662741</td>\n",
       "      <td>-3.060939</td>\n",
       "      <td>-1.932524</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1.116062</td>\n",
       "      <td>0.313836</td>\n",
       "      <td>0.270247</td>\n",
       "      <td>0.799881</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>-0.881938</td>\n",
       "      <td>0.311514</td>\n",
       "      <td>-0.739256</td>\n",
       "      <td>-0.666210</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>-0.900207</td>\n",
       "      <td>0.372292</td>\n",
       "      <td>-1.423042</td>\n",
       "      <td>-1.032613</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>-2.025591</td>\n",
       "      <td>0.753425</td>\n",
       "      <td>0.139548</td>\n",
       "      <td>-0.472791</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-0.002411</td>\n",
       "      <td>-1.686081</td>\n",
       "      <td>0.460911</td>\n",
       "      <td>-0.533697</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       adjSA1    adjSA2    adjSA3  adjSAtotal  Lv_1_Hi  Lv_2_Hi  Lv_3_Hi  \\\n",
       "269  0.143670  1.101324 -0.822506    0.117187        1        1        0   \n",
       "211 -0.783965 -0.834554  0.116317   -0.677632        0        0        0   \n",
       "197  0.475912 -0.162414  1.180378    0.804260        1        0        1   \n",
       "75  -0.088596  0.229732  0.972713    0.618738        0        1        1   \n",
       "177 -1.146056  0.662741 -3.060939   -1.932524        0        1        0   \n",
       "..        ...       ...       ...         ...      ...      ...      ...   \n",
       "188  1.116062  0.313836  0.270247    0.799881        1        1        1   \n",
       "71  -0.881938  0.311514 -0.739256   -0.666210        0        1        0   \n",
       "106 -0.900207  0.372292 -1.423042   -1.032613        0        1        0   \n",
       "270 -2.025591  0.753425  0.139548   -0.472791        0        1        0   \n",
       "102 -0.002411 -1.686081  0.460911   -0.533697        0        0        1   \n",
       "\n",
       "     Tot_Hi  \n",
       "269       0  \n",
       "211       0  \n",
       "197       1  \n",
       "75        1  \n",
       "177       0  \n",
       "..      ...  \n",
       "188       1  \n",
       "71        0  \n",
       "106       0  \n",
       "270       0  \n",
       "102       0  \n",
       "\n",
       "[243 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictors_df = df.iloc[:, 1:(df.shape[1] - 8)]\n",
    "outcomes_df = df.iloc[:, (df.shape[1] - 8):]\n",
    "\n",
    "display(predictors_df)\n",
    "display(outcomes_df)\n",
    "\n",
    "# Split into train and test\n",
    "predictors_train, predictors_test, outcomes_train, outcomes_test = train_test_split(predictors_df, outcomes_df, test_size = 0.2, random_state = 42)\n",
    "\n",
    "display(predictors_train)\n",
    "display(outcomes_train)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "predictors_train = scaler.fit_transform(predictors_train)\n",
    "predictors_train = pd.DataFrame(predictors_train, columns = predictors_df.columns)\n",
    "predictors_test = scaler.transform(predictors_test)\n",
    "predictors_test = pd.DataFrame(predictors_test, columns = predictors_df.columns)\n",
    "\n",
    "display(predictors_train)\n",
    "display(outcomes_train)\n",
    "\n",
    "# Free up memory\n",
    "del df\n",
    "del predictors_df\n",
    "del outcomes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model Regularization with Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def func_val_bin(predictors):\n",
    "    \"\"\"\n",
    "        Creates a function out of five variables in the predictors dataframe\n",
    "        and outputs a boolean Pandas series where True means the function value\n",
    "        was greater than or equal to the median and False otherwise.\n",
    "\n",
    "        Parameters:\n",
    "            predictors (Dataframe): pandas Dataframe containing all predictor features\n",
    "\n",
    "        Output:\n",
    "            (Series): pandas Series containing True and False values where True means that\n",
    "                      the calculated value was above the median and False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize arary to store continuous values of function\n",
    "    func_calcs = np.array([])\n",
    "\n",
    "    # Get random coefficients and features\n",
    "    random.seed(42)\n",
    "    coefficients = np.array([round(random.uniform(-5, 5), 1) for i in range(5)])\n",
    "    selected_features = np.array(random.sample(list(predictors.columns), 5))\n",
    "\n",
    "    print(\"Function = \" + \n",
    "            f\"{coefficients[0]} * {selected_features[0]} + \" +\n",
    "            f\"{coefficients[1]} * {selected_features[1]} + \" +\n",
    "            f\"{coefficients[2]} * {selected_features[2]} + \" +\n",
    "            f\"{coefficients[3]} * {selected_features[3]} + \" +\n",
    "            f\"{coefficients[4]} * {selected_features[4]}\")\n",
    "    \n",
    "    for index, row in predictors.iterrows():\n",
    "        # Add new calculation to func_calcs\n",
    "        func_calcs = np.append(func_calcs, (coefficients[0] * row[selected_features[0]] +\n",
    "                                              coefficients[1] * row[selected_features[1]] +\n",
    "                                              coefficients[2] * row[selected_features[2]] +\n",
    "                                              coefficients[3] * row[selected_features[3]] +\n",
    "                                              coefficients[4] * row[selected_features[4]]))\n",
    "\n",
    "    func_calcs_median = np.median(func_calcs)\n",
    "    return pd.Series(data = func_calcs >= func_calcs_median)\n",
    "\n",
    "def select_hyperparameters(model, predictors, outcome, params, eval_metric):\n",
    "    \"\"\"\n",
    "        Conducts GridSearchCV on a Logistic Regression model to identify suitable hyperparameters\n",
    "\n",
    "        Parameters:\n",
    "            model (sklearn Model): sklearn Model to conduct GridSearchCV on\n",
    "            predictors (Dataframe): pandas Dataframe containing all predictor features\n",
    "            outcome (Series): pandas Series containing all values for the outcome variable\n",
    "            params (dictionary): Dictionary of parameters for GridSearchCV for a LogisticRegression model\n",
    "            eval_metric (string): Name of evaluation metric to use for GridSearchCV\n",
    "\n",
    "        Return:\n",
    "            clf (GridSearchCV): GridSearchCV object after running GridSearchCV with provided parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Perform 5-fold cross-validation with different regularization strengths and regularization types\n",
    "    clf = GridSearchCV(model, params, cv = 5, scoring = eval_metric, n_jobs = -1)\n",
    "    clf.fit(predictors, outcome)\n",
    "\n",
    "    # Show the best regularization strength and penaalty type\n",
    "    print(\"Best regularization strength:\", clf.best_params_[\"C\"])\n",
    "    print(\"Best penalty:\", clf.best_params_[\"penalty\"])\n",
    "\n",
    "    if clf.best_params_[\"penalty\"] == \"elasticnet\":\n",
    "        print(\"Best alpha:\", clf.best_params_[\"l1_ratio\"])\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function = 1.4 * fNIRS_S8D6_hbr_timeToMax_version16 + -4.7 * fNIRS_S6D6_hbr_kurtosis_version19 + -2.2 * EEG_p100_poz_version12 + -2.8 * fNIRS_S7D5_hbo_kurtosis_version04 + 2.4 * fNIRS_S5D3_hbr_kurtosis_version12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "/home/jshen/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regularization strength: 0.1\n",
      "Best penalty: l1\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"C\": [0.01, 0.1, 1.0, 10.0],\n",
    "    \"penalty\": [\"l1\", \"elasticnet\"],\n",
    "    \"l1_ratio\": [.25, .50, .75]\n",
    "}\n",
    "\n",
    "SA_func_Log_Reg = LogisticRegression(solver = \"saga\", max_iter = 5000, fit_intercept = False)\n",
    "SA_GridSearchCV_func = select_hyperparameters(SA_func_Log_Reg, predictors_train, func_val_bin(predictors_train), params, \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8437074829931973)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SA_GridSearchCV_func.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function = 1.4 * fNIRS_S8D6_hbr_timeToMax_version16 + -4.7 * fNIRS_S6D6_hbr_kurtosis_version19 + -2.2 * EEG_p100_poz_version12 + -2.8 * fNIRS_S7D5_hbo_kurtosis_version04 + 2.4 * fNIRS_S5D3_hbr_kurtosis_version12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.1, fit_intercept=False, max_iter=20000, n_jobs=-1,\n",
       "                   penalty=&#x27;l1&#x27;, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=0.1, fit_intercept=False, max_iter=20000, n_jobs=-1,\n",
       "                   penalty=&#x27;l1&#x27;, solver=&#x27;saga&#x27;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.1, fit_intercept=False, max_iter=20000, n_jobs=-1,\n",
       "                   penalty='l1', solver='saga')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SA_model_func = LogisticRegression(max_iter = 20000, penalty = \"l1\", solver = \"saga\", C = 0.1, n_jobs = -1, fit_intercept = False)\n",
    "SA_model_func.fit(predictors_train, func_val_bin(predictors_train))\n",
    "SA_model_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(162)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.sum(SA_model_func.coef_ != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fNIRS_S8D6_hbr_timeToMax_version16</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fNIRS_S6D6_hbr_kurtosis_version19</th>\n",
       "      <td>-0.124784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EEG_p100_poz_version12</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fNIRS_S7D5_hbo_kurtosis_version04</th>\n",
       "      <td>-0.569231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fNIRS_S5D3_hbr_kurtosis_version12</th>\n",
       "      <td>0.502514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    coefficients\n",
       "fNIRS_S8D6_hbr_timeToMax_version16      0.000000\n",
       "fNIRS_S6D6_hbr_kurtosis_version19      -0.124784\n",
       "EEG_p100_poz_version12                  0.000000\n",
       "fNIRS_S7D5_hbo_kurtosis_version04      -0.569231\n",
       "fNIRS_S5D3_hbr_kurtosis_version12       0.502514"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SA_model_func_coef = pd.DataFrame(\n",
    "    data = {\n",
    "        \"coefficients\": SA_model_func.coef_[0]\n",
    "    },\n",
    "    index = np.array(list(predictors_test.columns))\n",
    ")\n",
    "\n",
    "SA_model_func_coef.loc[[\"fNIRS_S8D6_hbr_timeToMax_version16\", \"fNIRS_S6D6_hbr_kurtosis_version19\", \"EEG_p100_poz_version12\", \"fNIRS_S7D5_hbo_kurtosis_version04\", \"fNIRS_S5D3_hbr_kurtosis_version12\"], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show that the coefficients above where nowhere near their actual values. Slightly suspicious of the effectiveness of regularization, but reduced a good amount of the features without removing any of the actual features used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function = 1.4 * fNIRS_S8D6_hbr_timeToMax_version16 + -4.7 * fNIRS_S6D6_hbr_kurtosis_version19 + -2.2 * EEG_p100_poz_version12 + -2.8 * fNIRS_S7D5_hbo_kurtosis_version04 + 2.4 * fNIRS_S5D3_hbr_kurtosis_version12\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SA_func_predicted_outcomes = SA_model_func.predict(predictors_test)\n",
    "SA_func_cm = metrics.confusion_matrix(func_val_bin(predictors_test), SA_func_predicted_outcomes)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = SA_func_cm)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function = 1.4 * fNIRS_S8D6_hbr_timeToMax_version16 + -4.7 * fNIRS_S6D6_hbr_kurtosis_version19 + -2.2 * EEG_p100_poz_version12 + -2.8 * fNIRS_S7D5_hbo_kurtosis_version04 + 2.4 * fNIRS_S5D3_hbr_kurtosis_version12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f062586bd90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(func_val_bin(predictors_test), SA_func_predicted_outcomes)\n",
    "roc_auc  = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label = \"ROC Curve (area = %0.3f)\" % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], \"k--\") # Random predictions curve\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
